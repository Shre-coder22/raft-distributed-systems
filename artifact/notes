# `WEEK 1: Raft Introduction & Leader Election.` 

## üìòDAY 1: Raft Setup & Leader Election. 
### 1. ‚úÖ What I Did Today:
- Installed Go 1.19 using go install + go1.19 download
- Cloned and restructured MIT 6.824 repo (`raft`, `labgob`, `labrpc`)
- Set up Go modules (module name: `mitraft`)
- Replaced import paths (`labgob` ‚Üí `mitraft/labgob`)
- Fixed `GOROOT` and `go.mod` errors
- Successfully passed Lab 2A:
  - Test: initial election
  - Test: election after network failure

### 2.üõ†Ô∏è Debug Log & Fixes:
#### Errors:
- package labgob is not in GOROOT
- could not import labgob (no required module provides package...)

#### Fixes:
- Initialized module using `go1.19 mod init mitraft`
- Moved `labgob`, `labrpc`, and `raft` out of `src/` to root
- Replaced all import paths with `mitraft/xxx`
- Ran `go1.19 mod tidy` to sync dependencies

### 3.üìñ Raft Paper ‚Äì Section 3‚Äì4.1 Summary:
- Raft decomposes consensus into 3 subproblems:
  - Leader election
  - Log replication
  - Safety
- Leader election uses randomized timeouts to avoid split votes
- A candidate becomes leader after receiving a majority of votes
- Heartbeats prevent unnecessary elections

#### Diagram:
- Follower ‚Üí (timeout) ‚Üí Candidate ‚Üí (votes) ‚Üí Leader
- Term resets after each election cycle

### 4.üí≠ Reflections:
- Setting up Go modules taught me a lot about dependency resolution
- The lab is already more low-level and real than most courses
- Debugging GOROOT + import issues took time but I now understand Go‚Äôs module system much better
- Passing 2A was very satisfying ‚Äì saw logs of election working and leader being chosen

### 5. üñºÔ∏èScreenshots of test:

![[Screenshot 2025-06-28 152138.png]]



## üìòDAY-2: Election Logging, Timeout Tuning & Test 2A Pass.
### 1. ‚úÖ What I Did Today:

- üîß Updated and structured the `Run()` loop in `raft.go` (no test-breaking changes)
- üõ† Added tagged, color-coded `fmt.Printf` logs for:
  - Timeouts
  - Elections
  - Leader transitions
  - Step-down event
- üéØ Successfully passed `Test 2A` (initial election + re-election after failure)
- üìà Conducted **custom timeout experiments**:
  - Compared behavior with default (200‚Äì500ms) vs. higher (300‚Äì600ms) election timeouts
- üó∫Ô∏è Sketched out Raft‚Äôs **state transition diagram**
- - üé• Watched **MIT 6.824 Lecture 1** ‚Äî gave a strong overview of the course structure, research focus, and a conceptual foundation of the Raft protocol.

### 2.üß± Code Understanding:

#### ‚úÖ Goals Completed Today
- [x] Implemented safe logging inside `Run()` loop
- [x] Fixed term increment confusion (`term+1`)
- [x] Preserved original MIT control flow for safety
- [x] Validated behavior under normal and failure conditions
- [x] Captured clean logs for role transitions
- [x] All tests passed successfully

---

#### üß† Behavior Observed

#### üîπ Initial Election
```text
[‚è± TIMEOUT] Follower 2 ‚Üí Candidate (next term 1)
[üó≥Ô∏è ELECT] Candidate 2 starts election (term 1)
[1] becomes candidate (term 0)
[0] becomes candidate (term 0)
[üéØ LEADER] Node 2 won election ‚Üí Leader (term 1)
```

- Follower 2 timed out first and triggered an election.

- Despite other nodes becoming candidates, Node 2 secured a majority and won term 1.


---

#### üîπ Election After Network Failure

```
[‚è± TIMEOUT] Follower 2 ‚Üí Candidate (next term 1)
[üó≥Ô∏è ELECT] Candidate 2 starts election (term 1) 
[üéØ LEADER] Node 2 won election ‚Üí Leader (term 1) 
[‚è± TIMEOUT] Follower 1 ‚Üí Candidate (next term 2) 
[üó≥Ô∏è ELECT] Candidate 1 starts election (term 2) 
[üéØ LEADER] Node 1 won election ‚Üí Leader (term 2)  
[üó≥Ô∏è ELECT] Candidate 2 starts election (term 4)
[üó≥Ô∏è ELECT] Candidate 0 starts election (term 7) 
[üéØ LEADER] Node 2 won election ‚Üí Leader (term 8)
[‚¨áÔ∏è STEP-DOWN] Candidate 0 ‚Üí Follower (term 8) on heartbeat
```

- Multiple re-elections occurred due to term collisions and lack of majority.

- Raft eventually stabilized with a new leader in term 8.


---

#### üìä Test Results

| Test                      | Status   | Time  |
| ------------------------- | -------- | ----- |
| Initial Election          | ‚úÖ Passed | 2.5s  |
| Re-election After Failure | ‚úÖ Passed | 4.6s  |
| Total Run                 | ‚úÖ Passed | 7.67s |

### 3.üß™ Simulation:
#### üîç What‚Äôs Different vs Previous (200‚Äì500ms)

|Metric|200‚Äì500ms|300‚Äì600ms|Difference|
|---|---|---|---|
|üïí First timeout|Happens quickly (2s)|Happens later (after 2.5‚Äì3s)|Slower start|
|üîÅ Term escalations|~8 max|Up to 10|Slightly more|
|üß® Collisions|Slight, early|Delayed, but still present in mid-terms|More controlled start, same end|
|‚è≥ Test time|~7.67s|8.19s|0.5s longer|
|‚úÖ Leader election|Works in both|Works in both|Stable in both|


### 4.‚úçÔ∏è Notes + Diagram:

[Follower] --(timeout)--> [Candidate] --(wins vote)--> [Leader]
     ^                                |                    |
     |<--(heartbeat)------------------|<--(AppendEntries)--|
     |                                |
     |<--(loses or splits vote)-------|

## üìòDAY-3: Split Vote Debugging & Term Advancement Timing Analysis.

### 1. ‚úÖ What I Did Today:

- üß™ Ran and **analyzed output** of `TestReElection2A` in depth
    
- üîç Debugged **4 consecutive split votes** and identified timing patterns
    
- üìâ Created **timeline-based breakdown** of election events, with example timeout values per node
    
- üß† Understood **why Node 2 stepped down** in Term 8 ‚Äî due to receiving **RPC before becoming candidate**
    

---

### 2. üß± Deeper Raft Insights:

#### ‚úÖ Concepts Understood Today

-  Election collisions due to closely timed timeouts (within 5‚Äì10ms)
    
-  Followers can vote before starting election if RPC received earlier
    
-  Raft doesn't guarantee all nodes will become candidates in every term
    
-  Election resolution depends heavily on **timeout gaps and network delays**
    

---

#### üß† TestReElection2A Timeline Sample:

|Node|Timeout (ms)|Becomes Candidate @|Wins?|
|---|---|---|---|
|0|185|185ms (Term 4)|‚ùå|
|1|190|190ms (Term 4)|‚ùå|
|2|222|222ms (Term 5)|‚ùå|
|...|...|...|...|
|1|370|370ms (Term 8)|‚úÖ|
|2|‚Äî|RPC received before timeout|üó≥Ô∏è Voted for 1|

---

### 3. üß™ Simulation Notes:

#### üîç Why Split Votes Happened

- Timeouts of Node 0 and Node 1 were too close (e.g., 5ms gap)
    
- Both became candidates ‚Üí divided votes
    
- No majority formed ‚Üí new term begins
    

#### üìå Why Node 2 Stepped Down in Term 8

- Node 1 started election (Term 8)
    
- Node 2 was still follower (Term 7), hadn‚Äôt timed out
    
- Node 2 received valid `RequestVote` from Node 1 **before** becoming candidate
    
- As per Raft rules: granted vote + updated term ‚Üí became follower (Term 8)
    





## üìò DAY 4: Reset & Initialize Raft

### ‚úÖ 1. What I Did Today:

- Removed any previously reused/borrowed 2A code.
    
- Reinitialized the codebase with the **starter `raft.go`** file.
    
- Defined the **Raft struct** with all required **persistent and volatile state variables**.
    
- Set up the initial **RPC framework** with `RequestVote`.
    

### üß± 2. Code Understanding:

- Implemented `Raft` struct with:
    
    go
    
    CopyEdit
    
    `currentTerm int votedFor    int state       int // Follower, Candidate, Leader mu          sync.Mutex peers       []*labrpc.ClientEnd me          int electionResetEvent time.Time`
    
- Understood Go‚Äôs `sync.Mutex` for concurrent access.
    
- Setup of `Make()` function with proper initialization.
    

### üî¨ 3. Simulation:

- Printed logs to verify Raft instances starting with term `0`, `state=Follower`, and `votedFor=-1`.
    

### ‚úçÔ∏è 4. Notes + Diagram:

- Simple Raft State Machine Diagram:
    
    rust
    
    CopyEdit
    
    `Follower ---> Candidate ---> Leader      ^           |            |      |___________|____________|          (Timeout or RPC Term)`
    
- Noted how nodes become candidates upon election timeout.
    

---

## üìò DAY 5: Implement `RequestVote`

### ‚úÖ 1. What I Did Today:

- Defined `RequestVoteArgs` and `RequestVoteReply`.
    
- Wrote the full `RequestVote()` RPC handler with locking and term update logic.
    
- Implemented `startElection()` function that is triggered on timeout.
    

### üß± 2. Code Understanding:

- `RequestVoteArgs` includes:
    
    go
    
    CopyEdit
    
    `Term         int CandidateId  int`
    
- `RequestVote()` handler:
    
    - Grants vote if term is up-to-date and hasn‚Äôt voted yet.
        
    - Steps down to Follower if request term > currentTerm.
        
- `startElection()`:
    
    - Increments term.
        
    - Votes for self.
        
    - Sends concurrent `RequestVote` RPCs to all peers.
        
    - Collects votes via goroutines + locking.
        

### üî¨ 3. Simulation:

- Triggered elections manually using `ticker()`.
    
- Verified term updates and vote counts in console:
    
    csharp
    
    CopyEdit
    
    `[Node 0] Starting election for term 1 [Node 2] voted for 0 in term 1 [Node 0] got vote from 2 for term 1 (votes: 2)`
    

### ‚úçÔ∏è 4. Notes + Diagram:

- Voting Conditions:
    
    sql
    
    CopyEdit
    
    `if args.Term < currentTerm ‚Üí Reject if votedFor == -1 or votedFor == args.CandidateId ‚Üí Grant Vote`
    

---

## üìò DAY 6: Complete Election Logic + Ticker

### ‚úÖ 1. What I Did Today:

- Completed `startElection()` to correctly elect a leader with majority votes.
    
- Added `ticker()` function to:
    
    - Track time since last heartbeat/election.
        
    - Call `startElection()` on timeout.
        
- Added goroutine to periodically check for timeouts using `time.Since()`.
    

### üß± 2. Code Understanding:

- `ticker()` loop:
    
    go
    
    CopyEdit
    
    `for !rf.killed() {     time.Sleep(10 * time.Millisecond)     if time.Since(rf.electionResetEvent) >= timeout {         startElection()     } }`
    
- Ensured each server has **random election timeouts** between 600‚Äì1000ms.
    
- Introduced **heartbeat goroutine** (even if empty) to simulate AppendEntries for leader.
    

### üî¨ 3. Simulation:

- Tested scenarios where multiple nodes start elections.
    
- Ensured only one leader is elected due to randomized timeout.
    
- Verified log output:
    
    rust
    
    CopyEdit
    
    `Node 2 becomes leader for term 11`
    

### ‚úçÔ∏è 4. Notes + Diagram:

- Heartbeat Frequency: every 100ms (‚â§ 10/sec)
    
- Election Timeout Range: 600ms‚Äì1000ms (customized from 150ms‚Äì300ms)
    
- Leader Election Timing:
    
    sql
    
    CopyEdit
    
    `Term Increases ‚Üí Candidate starts election ‚Üí Sends RequestVote RPCs ‚Üí Collect votes ‚Üí Becomes Leader`
    

---

## üìò DAY 7: Final Fixes + Pass Test 2A

### ‚úÖ 1. What I Did Today:

- Ensured `GetState()` was thread-safe using `mu.Lock()`.
    
- Fixed `RequestVote` race conditions and stale term handling.
    
- Debugged test failures like:
    
    sql
    
    CopyEdit
    
    `config.go:331: expected one leader, got none`
    
- Final run: **Passed `go test -run 2A`** ‚úÖ
    

### üß± 2. Code Understanding:

- Verified conditions in `startElection()`:
    
    - Check `rf.currentTerm == termAtStart`
        
    - Only increment vote count if state is still Candidate.
        
- Removed `matchIndex` and `nextIndex` (only needed in 2B).
    
- Used goroutines for `sendRequestVote`.
    

### üî¨ 3. Simulation:

- Captured console logs of successful elections:
    
    bash
    
    CopyEdit
    
    `Node 2 becomes leader for term 11 PASS ok  	mitraft/raft	8.562s`
    

### ‚úçÔ∏è 4. Notes + Diagram:

- Heartbeats: periodic empty `AppendEntries` RPCs (just Term + LeaderID)
    
- Leader sends heartbeats every 100ms:
    
    go
    
    CopyEdit
    
    `for rf.killed() == false {     for peer := range rf.peers {         if peer != rf.me {             sendAppendEntries()         }     }     time.Sleep(100ms) }`
    

---




#  üì¶`WEEK 2: Log Replication Inferno`

## üìò DAY 8: Log Structure, Start(), and sendAppendEntries() Skeleton

### 1. ‚úÖ What I Did Today:
    
    - Implemented `Start(command)` function to append entries as leader.
        
    - Built `sendAppendEntries()` structure (skeleton).
        
    - Created placeholder structs for `AppendEntriesArgs/Reply`.
        
### 2. üß± Code Understanding:
    
    - Understood when and how new entries are added by a leader.
        
    - Difference between `matchIndex`, `nextIndex`, and `len(rf.log)`.
        
### 3. üìè Simulation:
    
    - Single leader scenario with one client sending commands.
        
    - Simulated send/receive RPCs using dummy logs.
        
### 4. ‚úçÔ∏è Notes + Diagram:
    
    - Log replication flowchart.
        
    - Table: Log structure before and after replication.
        
    - Defined corner cases like empty AppendEntries (heartbeat).
        

---

## üìò DAY 9: Follower Consistency Checks + Log Matching

### 1. ‚úÖ What I Did Today:
    
    - Implemented core logic in `AppendEntries()` RPC.
        
    - Wrote logic to detect mismatches (PrevLogIndex/Term checks).
        
    - Implemented overwrite/truncation for conflict resolution.
        
### 2. üß± Code Understanding:
    
    - Deep dive into log matching property.
        
    - Identified why followers must reject wrong logs.
        
    - Understood rollback using `ConflictIndex`.
        
### 3. üß™ Simulation:
    
    - Ran cases where leader log diverged from follower‚Äôs.
        
    - Validated rejection + truncation + appending new entries.
        
### 4. ‚úçÔ∏è Notes + Diagram:
    
    - Conflict resolution loop diagram.
        
    - Table: `args.PrevLogIndex`, `args.PrevLogTerm`, follower term comparison.
        
    - Added test entry flow with PrevLogIndex shifts.
        

---

## üìò DAY 10: MatchIndex Updates + CommitIndex Advancement

### 1. ‚úÖ What I Did Today:
    
    - Updated `matchIndex[]`, `nextIndex[]` logic on success.
        
    - Added commit rule: majority match + current term check.
        
    - Refined condition to apply logs to state machine.
        
### 2. üß± Code Understanding:
    
    - Identified when and why leader should advance `commitIndex`.
        
    - Matched term condition avoids stale log commit.
        
### 3. üß™ Simulation:
    
    - Ran 5-node scenarios with partially replicated logs.
        
    - Verified only majority logs with current term committed.
        
### 4. ‚úçÔ∏è Notes + Diagram:
    
    - Flowchart: MatchIndex collection ‚Üí CommitIndex bump.
        
    - Table: All servers' matchIndex and majority logic.
        

---

## üìò DAY 11: Final Fixes + Pass TestBasicAgree2B

### 1. ‚úÖ What I Did Today:
    
    - Fixed panic due to uninitialized logs (dummy entry logic).
        
    - Corrected boundary checks in AppendEntries.
        
    - Ensured correct use of `len(rf.log)` vs `log indices`.
        
### 2. üß± Code Understanding:
    
    - Realized how offset errors break term comparison.
        
    - Difference between actual log index vs array index.
        
### 3. üß™ Simulation:
    
    - Ran all 2B tests (`BasicAgree`, `FailAgree`, `RPCBytes`, etc).
        
    - Edge cases: follower log too short, term mismatch mid-log.
        
### 4. ‚úçÔ∏è Notes + Diagram:
    
    - Error tracebacks ‚Üí bug ‚Üí fix documented.
        
    - Diagram: Leader sends entries, follower truncates + applies.
        
    - Added explanation of `sendAppendEntries` safe return conditions.

## üìò DAY 12: Fix TestRPCBytes2B & Refactor startElection

### ‚úÖ What I Did Today:

- Fixed **`TestRPCBytes2B`**, which was failing due to excessive RPC retries and log retransmissions.
    
- Added `[SEND]` / `[RECV]` debug logs inside `broadcastAppendEntries()` and `sendAppendEntries()` for traceability.
    
- Refactored monolithic `startElection()` function into:
    
    - `sendRequestVote()` ‚Äì handles individual peer vote requests.
        
    - `broadcastRequestVote()` ‚Äì sends votes to all peers.
        
- Added conditional check:
    
    go
    
    CopyEdit
    
    `if rf.matchIndex[peer] == lastIndex { return }`
    
    to avoid sending unnecessary log entries to up-to-date followers.
    

### üß± Code Understanding:

- Learned that the **test checks RPC byte count**, not just correctness.
    
- Understood the importance of preventing **duplicate entries** being sent.
    
- Carefully analyzed `nextIndex[]` and `matchIndex[]` logic to optimize log replication.
    
- Found that some redundant RPCs were due to not checking `rf.matchIndex[peer]` before resending.
    

### üß™ Simulation:

- Enabled print logs to trace every `AppendEntries` call.
    
- Verified that followers were already in sync, yet leader was resending due to bad conditions.
    
- Walked through what each peer would receive and how `matchIndex` was being updated.
    

### ‚úçÔ∏è Notes + Diagram:

- Refactored election logic into two helper methods for testability and clarity.
    
- Documented the `[SEND]/[RECV]` format to debug other RPC-heavy tests.
    
- Updated log consistency diagrams in notes to account for redundant sends.
    

---

## üìò DAY 13: Full Log Replication Logic + All Tests Passed !!

### ‚úÖ What I Did Today:

- Completed the **entire remaining logic** for log replication and commitment.
    
- Fixed:
    
    - Log conflict detection in follower (`AppendEntries` reply logic).
        
    - Truncation + appending strategy for mismatched logs.
        
- Wrote helper method `findLastMatchIndex()` for majority commit index.
    
- Passed all 2B test cases including:
    
    - `TestBasicAgree2B`
        
    - `TestFailAgree2B`
        
    - `TestFailNoAgree2B`
        
    - `TestConcurrentStarts2B`
        
    - `TestRejoin2B`
        
    - `TestBackup2B`
        
    - `TestCount2B`
        
    - ‚úÖ **`TestRPCBytes2B` (Already done Day 12)**
        

### üß± Code Understanding:

- Deep dived into how followers handle conflicts:
    
    - Check if `args.PrevLogIndex` exists.
        
    - Match `args.PrevLogTerm` with local log term.
        
    - Truncate log from the conflicting index if mismatch detected.
        
- Learned how majority-based commit is determined using `matchIndex[]`.
    

### üß™ Simulation:

- Manually tested:
    
    - Case where one follower is behind ‚Üí ensure proper truncation + re-append.
        
    - Leader crash and recovery ‚Üí logs remain consistent.
        
- Validated term updates, log overwrite behavior, and commit advancement with diagrams.
    

### ‚úçÔ∏è Notes + Diagram:

- Added flowchart for `AppendEntries` on follower:
    
    - Term check ‚Üí Index check ‚Üí Term match ‚Üí Conflict handling ‚Üí Log update.
        
- Snapshot of updated `matchIndex` and `nextIndex` tracking in Obsidian.
    
- Clearly documented leader commit advancement logic using `N > commitIndex` rule.

## üìò DAY 14: Implement `persist`, `readPersist`, Debugging + Test Passes (2C)

### ‚úÖ 1. What I Did Today:

- Implemented `persist()` and `readPersist()` for crash recovery.
    
- Integrated persistence in key Raft transitions (vote/term/log updates).
    
- Refactored code to ensure `persist()` is called exactly where state changes matter.
    
- Passed all 2C tests **except `TestFigure82CUnreliable`**.
    
- Improved logs and comments around state restoration and crash recovery.
    

---

### üõ†Ô∏è 2. Debug Log & Fixes:

- Fixed edge case where `readPersist()` didn‚Äôt correctly restore logs due to shallow copy.
    
- Added debug prints to verify:
    
    - Log restoration after crash.
        
    - `currentTerm`, `votedFor` restoration.
        
- Made sure ApplyMsgs were consistent after recovery.
    
- Reduced duplicated `persist()` calls in election/append logic.
    

---

### üìÑ 3. Raft Paper Tie-in:

- Section **5.2: Persistence** studied in detail.
    
- Realized importance of storing both `currentTerm`, `votedFor`, and log entries to ensure no inconsistent log replay.
    
- Clarified when to **write** and **read** persistent state.
    

---

### üí≠ 4. Reflections:

- Persistence is minimal in Raft but **high-impact**.
    
- A single missed `persist()` can lead to failed recoveries and obscure bugs.
    
- Reading the persisted log on `Make()` call is crucial and affects leader election stability.
    

---



#  üì¶ `WEEK 3: Dashboard & Commit Agreement`

## üìò DAY 15: Dashboard Setup, UI Design, Pentagon Layout, Components

### ‚úÖ 1. What I Did Today:

- Started Raft Dashboard frontend (React + Tailwind).
    
- Designed node layout in a **perfect regular pentagon** with state-based coloring.
    
- Built individual `<Node />` component for Raft state.
    
- Implemented `initialNodes.js` with radial layout logic using angle math.
    
- Created layout container, center alignment, and placeholder buttons (Start, Pause, Reset).
    

---

### üé® 2. UI Design Decisions:

- Used `relative` positioning with `top/left` in `%` to allow responsive pentagon layout.
    
- Color-coded nodes:
    
    - üîµ Follower
        
    - üü° Candidate
        
    - üî¥ Leader
        
- Added `transform: translate(-50%, -50%)` to center nodes exactly.
    
- Button bar with `flex space-x` for future simulation control.
    

---

### üß© 3. Components Built:

- `Node.jsx`: Renders a Raft node with ID, state, term, and color.
    
- `App.jsx`: Maps over `initialNodes` to layout nodes.
    
- `initialNodes.js`: Calculates regular pentagon node positions using polar coordinates.
    

---

### ‚úèÔ∏è 4. Notes + Diagram:

- Verified angular spacing: angle = 2œÄiN‚àíœÄ2\frac{2\pi i}{N} - \frac{\pi}{2}N2œÄi‚Äã‚àí2œÄ‚Äã for top alignment.
    
- Created modular setup where UI can later plug into backend simulation.
    
- Setup ensures every node can be individually updated visually.
## üìò DAY 16: Node Interactivity, Click Events & Prep for Details Panel

### ‚úÖ What I Did Today:

- Extended `<Node />` component to handle **click events**.
    
- Added state hooks in `App.jsx` for `selectedNode` to track which node was clicked.
    
- Built the basic framework for opening node details in a future pop-up panel.
    
- Ensured each node still renders in **pentagon layout** with correct positioning.
    

---

### üé® UI Additions:

- Hover + click effect on nodes (`cursor-pointer`, hover highlight).
    
- Node IDs now act as selectors ‚Üí clicking saves the `nodeId` to state.
    

---

### üß© Components:

- Updated **Node.jsx** to accept `onClick`.
    
- `App.jsx` now has:
    
    `const [selectedNode, setSelectedNode] = useState(null);`
    

---

### ‚úèÔ∏è Notes:

- This prepared the ground for the **node details modal**, without breaking step slider / polling sync.
    
- Verified that clicking each node registers properly in React state.
    

---

## üìò DAY 17: Node Details Pop-Up Modal (UI Only)

### ‚úÖ What I Did Today:

- Built `NodeDetailsModal.jsx` as a reusable pop-up component.
    
- Modal displays:
    
    - Node ID
        
    - Role (Leader / Follower / Candidate)
        
    - Current term
        
    - Commit index
        
    - Logs (in list format)
        
- Added **close button (√ó)** ‚Üí resets `selectedNode` to null.
    

---

### üé® UI Design Decisions:

- Used fixed fullscreen overlay with semi-transparent dark background.
    
- Centered white card with Tailwind (`rounded-lg shadow-lg p-6`).
    
- Scrollable log section for future long logs.
    

---

### üß© Components:

- `NodeDetailsModal.jsx` created.
    
- Integrated into `App.jsx` ‚Üí conditionally renders if `selectedNode !== null`.
    

---

### ‚úèÔ∏è Notes:

- Modal is purely UI-driven today (fake data injected).
    
- Verified open/close cycle works seamlessly.
    

---

## üìò DAY 18: Real Node Click ‚Üí Show Actual Node Data

### ‚úÖ What I Did Today:

- Connected modal to **real node data** from backend polling.
    
- On click ‚Üí find node in `raftState.nodes[]` by ID ‚Üí pass down props to `NodeDetailsModal`.
    
- Logs are now shown per-node based on history snapshot at current step.
    

---

### üß© Code Changes:

- In `App.jsx`:
    
    `const node = nodes.find(n => n.id === selectedNode); <NodeDetailsModal node={node} onClose={() => setSelectedNode(null)} />`
    
- Modal now shows **live state** (not hardcoded).
    

---

### üß™ Simulation:

- Tested across 4 steps:
    
    - Step 0 ‚Üí logs empty.
        
    - Step 1 ‚Üí leader shows `[x=1]`.
        
    - Step 2 ‚Üí followers catch up.
        
    - Step 3 ‚Üí `[x=1, y=5]`.
        

---

### ‚úèÔ∏è Notes:

- This makes modal fully functional for history playback.
    
- Critical milestone: **click ‚Üí inspect ‚Üí close** loop completed.
    

---

## üìò DAY 19: Timeline & Step Slider Integration with Modal

### ‚úÖ What I Did Today:

- Ensured modal reflects **historical node states** via step slider.
    
- If slider moves ‚Üí modal updates instantly with correct snapshot.
    
- Added syncing with play/pause ‚Üí modal live-updates when simulation runs.
    

---

### üß© Components:

- `StepSlider.jsx` + `App.jsx` ‚Üí pass `currentStep` into modal.
    
- Modal reads data from `nodesHistory[currentStep]`.
    

---

### üß™ Simulation:

- Open modal at Step 3 ‚Üí shows logs `[x=1, y=5]`.
    
- Move slider back to Step 1 ‚Üí instantly re-renders logs `[x=1]`.
    

---

### ‚úèÔ∏è Notes:

- First **end-to-end playback + inspection** workflow achieved.
    
- This locks in our Week 3 core milestone.
    

---

## üìò DAY 20: Backend Prep for Fault Injection

### ‚úÖ What I Did Today:

- Shifted focus backend-side to prep for **fault injections**.
    
- Reviewed `backend.js` and `cluster.js`.
    
- Decided to keep **20‚Äì25 default steps static**.
    
- Dynamic steps will only be appended when user triggers faults (partition, drop, crash).
    

---

### üß© Backend Plan:

- Add new API routes:
    
    - `POST /raft/fault` ‚Üí injects fault event.
        
    - `GET /raft/steps` ‚Üí returns updated step history (default + dynamic).
        
- In `cluster.js`:
    
    - Maintain `steps[]` default history.
        
    - Fault injections ‚Üí push into `steps[]` dynamically.
        

---

### ‚úèÔ∏è Notes:

- Made sure not to delete existing REST routes (`/raft/state`, `/raft/reset`).
    
- ESM + function-based only ‚Üí no class.
    

---

## üìò DAY 21: Backend Cluster.js Functional Rewrite (Step Logic)

### ‚úÖ What I Did Today:

- Began refactoring `cluster.js` into **function-based module**.
    
- Core exports:
    
    - `getFilteredState(step)`
        
    - `resetToInitialState()`
        
    - `injectFault(type)`
        
    - `getSteps()`
        
- Removed class-based approach.
    
- State kept in module-level variables (`let steps = []; let currentStep = 0;`).
    

---

### üß© Code Example:

`let steps = [...defaultSteps]; let currentStep = 0;  export const getFilteredState = step => steps[step] || steps[0]; export const resetToInitialState = () => { steps = [...defaultSteps]; currentStep = 0; }; export const injectFault = type => { /* modify steps + push new */ };`

---

### ‚úèÔ∏è Notes:

- This ensures **functional, ESM-aligned design** ‚Üí consistent with frontend.
    
- Dynamic steps ready for fault injection starting Week 4.
    
- Current state: Backend + frontend are aligned with **default ‚Üí dynamic expansion** plan.

# üì¶ `WEEK 4: Fault Simulations + Experiments`

## üìò DAY 22: Crash/Recover Node, Timeout Forcing

### ‚úÖ What I Did Today:

- Began implementation of **fault simulation features** for the Raft dashboard.
    
- Added actions to **crash a node** and **recover a node** via control buttons.
    
- Added logic to **force timeout** a follower, pushing it into Candidate state.
    

---

### üß© Components Built/Updated:

- Extended **ControlPanel** with new fault buttons: Crash, Recover, Force Timeout.
    
- Updated **Node.jsx** to visually reflect crashed state (gray overlay / inactive).
    
- Added handlers in `App.jsx` to process crash/recover actions and propagate state updates.
    

---

### üé® UI Updates:

- Crashed node marked with **dimmed circle & disabled interactivity**.
    
- Added button hover states for clarity.
    
- Separated normal control buttons (Play, Pause, Reset) from fault injection actions.
    

---

### üêû Debug & Fixes:

- Initial issue: crashed node kept sending heartbeats ‚Üí fixed by excluding crashed nodes from polling updates.
    
- Timeout forcing initially reset instantly ‚Äî fixed by enforcing randomized election delay on triggered node.
    

---

---

## üìò DAY 23: Leader Crash & Election Flow

### ‚úÖ What I Did Today:

- Implemented **leader crash behavior** ‚Üí once leader is killed, system enters election phase.
    
- Randomized election timeouts among followers and selected highest-term candidate.
    
- Ensured election resumes with new leader correctly broadcasting heartbeats.
    

---

### üß© Components Built/Updated:

- Election logic integrated in **simulation backend** (inside step updates).
    
- Added **Election** visual marker: candidate nodes highlighted in yellow.
    
- Adjusted **StepSlider** so elections could be visualized across steps.
    

---

### üêû Debug & Fixes:

- Bug: after leader crash, multiple leaders spawned. Fixed by ensuring **only nodes with up-to-date logs** were eligible.
    
- Bug: term mismatch caused nodes to reject vote requests ‚Äî fixed by syncing replicated logs before allowing candidacy.
    
- Added console logging during debugging to verify election winner consistency.
    

---

### üî¨ Simulation Changes:

- Heartbeats stop immediately after leader crash.
    
- Election begins only among alive nodes with fully replicated logs.
    
- New leader resumes sending heartbeats ‚Üí log replication restored.
    

---

---

## üìò DAY 24: Drop-Log Action, Replication Refinement

### ‚úÖ What I Did Today:

- Added **Drop Latest Log Entry** feature per node for testing log consistency.
    
- Refined log replication: leader continuously appends and re-syncs missing logs on followers.
    

---

### üß© Components Built/Updated:

- New **Drop Log button** per node inside Node Details popup.
    
- Extended **LogsPanel** to show updates after log drop + resync.
    
- Updated **MessageArrow** to visually represent AppendEntries after drops.
    

---

### üé® UI Updates:

- Improved Node Details modal with **logs list** and action buttons (drop log, recover).
    
- Added **color-coded log entries** for easy tracking of dropped vs replicated entries.
    

---

### üêû Debug & Fixes:

- Bug: dropped logs not syncing properly after leader append ‚Üí fixed by checking `prevLogIndex` before replication.
    
- Bug: logs duplicated across steps when replaying history ‚Äî solved by ensuring **logs are cumulative but non-duplicating**.
    
- Fixed arrows disappearing after log drop ‚Üí added re-render triggers on message sending.
    

---

---

## üìò DAY 25: Final Debugging & Experiment Prep

### ‚úÖ What I Did Today:

- Focused on **debugging disappearing messages and logs**.
    
- Fixed rendering issues where **balls/arrows disappeared** due to missing state in history.
    
- Stabilized polling + step slider sync for fault simulation scenarios.
    

---

### üêû Debug & Fixes:

- Major issue: messages vanished after step replay. Fixed by clarifying that **only node states are stored in history, not transient messages**.
    
- Debugged with **console.logs (clgs)** to verify transitions step-by-step.
    
- Removed failed patches that caused arrows/balls to disappear globally.
    
- Final UI stabilized: no ghost bugs, no message dropouts.
    

---

### üß© Components Built/Updated:

- Updated `useRaftPolling` to sync correctly with new fault injection states.
    
- Clean separation of **persistent history (node states)** vs **ephemeral simulation (messages)**.
    
- Refined **ControlPanel** to ensure button triggers do not conflict with slider playback.
    

---

### üî¨ Simulation Changes:

- Leader-only heartbeats enforced until leader crash.
    
- Election resets replicated logs, ensuring **strong log consistency across leader transitions**.
    
- Crash/recover + drop log actions verified across all nodes.
## üìò DAY 26: Log Replication, Crashes & Stability Testing

### ‚úÖ 1. What I Did Today

- Ran extended simulations with **leader crash + recovery** scenarios.
    
- Observed system behavior with log replication under stress.
    
- Introduced **log trimming** (drop last entry) to simulate partial data loss.
    

### üß© 2. Simulation Changes

- Added option to **force crash** any node.
    
- Leader crashes triggered **Election Mode** ‚Üí two eligible candidates selected (alive, latest replicated logs).
    
- Implemented **randomized election timeout** among candidates to simulate real Raft election.
    
- Added **drop latest log entry** action per node to test consistency guarantees.
    

### üñ•Ô∏è 3. UI Updates

- Crash/recover buttons added to node details popup.
    
- Election process now clearly shown by **state color change** (Candidate ‚Üí Leader).
    
- Log display updated to immediately reflect dropped entries.
    

### üêõ 4. Debug & Fixes

- Fixed bug where recovering node re-joined with stale term. Now synced to leader‚Äôs term.
    
- Solved UI issue: sometimes node color stuck on Candidate after election ‚Üí corrected via state reset hook.
    

---

## üìò DAY 27: Failover Metrics & Latency Analysis

### ‚úÖ 1. What I Did Today

- Collected data on **failover time, leader stability, and replication latency**.
    
- Built repeatable simulation runs with different sequences of crash/recover + log drop.
    
- Ensured dashboard properly records history for later analysis.
    

### üß© 2. Simulation Changes

- Added internal timers to record:
    
    - ‚è±Ô∏è Time between leader crash and new leader election.
        
    - üì¶ Latency of log replication to all followers.
        
- Extended step history to store event metadata.
    

### üñ•Ô∏è 3. UI Updates

- Timeline slider improved with **event annotations** (crash, recover, election).
    
- Added subtle highlight on node logs when new entry replicated.
    
- Message arrows adjusted to show faster replication speed vs slower catch-up.
    

### üêõ 4. Debug & Fixes

- Fixed race condition where both candidates sometimes declared themselves leader. Now ensures **single leader commit**.
    
- Corrected mismatch in step history where follower log display lagged by one step.
    

---

## üìò DAY 28: Data Plotting & Experiment Wrap-Up

### ‚úÖ 1. What I Did Today

- Exported recorded metrics (failover time, replication delay) to CSV for later graphing.
    
- Verified all dashboard experiments run without UI/logic errors.
    
- Closed the loop on **Week 4 plan** ‚Üí simulations, crash handling, log drops, elections all stable.
    

### üß© 2. Components Built/Updated

- Added lightweight **metrics exporter** in backend.
    
- Updated StepSlider to stay synced even after rapid crash/recover events.
    
- Refined **Node Log display** to support variable log lengths.
    

### üñ•Ô∏è 3. UI Updates

- Improved arrow rendering for multiple simultaneous AppendEntries ‚Üí grouped arrows per leader step.
    
- Cleaned node popup design for better readability of crash/recover actions.
    

### üêõ 4. Debug & Fixes

- Solved final rendering issue: static simulation ‚Äúballs‚Äù (message arrows) not appearing ‚Üí fixed by always resetting message layer on step change.
    
- Minor polish: ensured reset button clears both history + metrics buffer.

