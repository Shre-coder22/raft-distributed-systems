---
title: "Making Raft Visible: an instrumented consensus implementation with a fault-injectable dashboard"
tags:
  - distributed systems
  - consensus
  - raft
  - measurement
  - reproducibility
authors:
  - name: Shrestha Saxena
    orcid: 0009-0005-2254-4804
    affiliation: 1
affiliations:
  - name: Department of Computer Science and Engineering, Rajiv Gandhi Institute of Petroleum Technology (RGIPT), Jais, Uttar Pradesh, India
    index: 1
date: 2025-09-09
bibliography: paper.bib
---

# Summary
Raft is a leader-based consensus protocol for state machine replication: a cluster elects a single leader via randomized timeouts; clients send operations to the leader, which appends them to a replicated log and commits entries once a majority acknowledges. Safety is ensured by leader election and log–matching rules; liveness is maintained under crash faults and unreliable networks through heartbeats and timeouts. Raft targets the same fault model and efficiency as Multi-Paxos while being simpler to understand and implement [@ongaro2014; @lamport1998].

Raft is widely taught, yet its timing behavior under failures is rarely quantified outside production systems. This package provides a compact, instrumented Raft implementation plus a browser dashboard for **fault injection** (leader crashes, packet loss) and **ground-truth logging** (election start, leader elected, first heartbeat, replication commit). The toolkit turns classroom Raft into a **measurable system**: users run scripted experiments and regenerate all plots from CSV metrics.

Using a 5-node cluster with normalized timers (heartbeat ≈ 100 ms; randomized election timeout 240–400 ms), we reproduce a long-run stability experiment (100 induced failovers; 77 valid after cleaning) and compare against a prior 40-event run. We observe low typical failover with **heavy tails** (median 2.29 s; p95 9.01 s); short runs **underestimate** tails. Leadership is balanced (≈14–17 terms per node). Replication latency remains low at the median (≈0.28–0.47 s) while p95 rises with drop and stays <1 s across tested rates. All figures are regenerated by the included analyzer.

# Statement of need
Researchers and instructors have visual demos of Raft, but lack **open, reproducible** tools to **measure** failover and replication under controlled churn. This package fills that gap with (i) an instrumented Raft core, (ii) a UI that injects faults and records events, and (iii) an analysis pipeline that yields CDFs, tenure distributions, and latency-vs-loss plots. It targets courses, benchmarking, and practitioners exploring timeout/SLO trade-offs in consensus systems [@ongaro2014; @lamport1998].

# State of the field
Production Raft systems (e.g., etcd) and formal verification frameworks (e.g., Verdi) emphasize **correctness** and safety proofs [@etcd; @wilcox2015]. Educational visualizations (e.g., Stanford’s Raft viz; MIT 6.824 labs) illustrate protocol dynamics but generally lack **ground-truth metrics** and **reproducible long-run** experiments [@stanfordviz; @mit6824]. Our contribution is a lightweight **measurement pipeline** complementary to these efforts.

# Functionality and quality control
The repository includes:
- **Instrumented Raft** with event hooks for elections, heartbeats, and replication;
- **Dashboard** to inject crashes and packet-drop rates and to visualize state;
- **Metrics logger** writing CSVs (`failover_trials.csv`, `replication_latency.csv`, `leader_tenure.csv`);
- **Analyzer** (`raft_experiments/analyze_raft_results.py`) that regenerates all figures;
- **Tests** for leader election and log-matching invariants (`go test`, e.g., `go test -run 2A`);
- **CI** to run tests and lint (workflow provided).

_Reproducibility._ Timers in the UI are slowed for pedagogy; analysis normalizes by 25× to reflect typical practice (heartbeat ≈100 ms; election timeout 240–400 ms), preserving ratios and Raft dynamics. Scripts, fixed seeds, and deterministic crash schedules reproduce all figures.

# Installation & quick start
```bash
# requirements: Go 1.19+, Node 20+, Python 3.10+
# run dashboard
cd raft-dashboard/server && npm run dev
cd ../client && npm run dev    # in another shell

# analyze a recorded run (writes figures under ./metrics_100run/figures)
py raft_experiments/analyze_raft_results.py --input ./metrics_100run --out ./metrics_100run/figures 
```

# References